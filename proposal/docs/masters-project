\documentclass[11pt, oneside]{article}   	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}

\usepackage{geometry}                	
\geometry{a4paper}                   	

\usepackage{graphicx}	
\usepackage{float}
\usepackage{afterpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes, calc, shapes, arrows, positioning}

\usepackage{amsthm}
\usepackage{amsmath}							
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage[utf8]{inputenc}

\usepackage{natbib}

\usepackage{url}

\usepackage{commath}

\usepackage[super]{nth}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thetab}{{\boldsymbol{\theta}}}
\newcommand{\thetaHat}{{\hat{\thetab}}}
\newcommand{\alphab}{{\boldsymbol{\alpha}}}
\newcommand{\betab}{{\boldsymbol{\beta}}}
\newcommand{\mub}{\boldsymbol{\mu}}
\newcommand{\Sigmab}{\boldsymbol{\Sigma}}
\newcommand{\xib}{{\boldsymbol{\xi}}}
\newcommand{\xitildeb}{{\boldsymbol{\tilde{\xi}}}}
\newcommand{\pnorm}{p}
\newcommand{\pnn}{\phi}
\newcommand{\pnnj}{f}
\newcommand{\pdata}{p_{ \mathbf x}}
\newcommand{\pnoise}{p_{ \mathbf y}}
\newcommand{\pcondnoise}{p_{\y|\x}}
\newcommand{\palpha}{p_\alphab}
\newcommand{\pbeta}{p_\betab}
\newcommand{\q}[1]{q(\z|{#1})}
\newcommand{\gradtheta}{\nabla_{\thetab}}

\newcommand{\Ib}{\boldsymbol{I}}
\renewcommand{\u}{{\mathbf u}}
\newcommand{\x}{{\mathbf x}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\ytilde}{\mathbf{\tilde{y}}}
\newcommand{\X}{{\mathbf X}}
\newcommand{\psib}{\boldsymbol{\psi}}


\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ex}{\E_{\x}}
\newcommand{\Ey}{\E_{\y}}
\newcommand{\Edata}{\E_{\x \sim p^*}}
\newcommand{\Evar}[1]{\E_{\z \sim \q{#1}}}
\newcommand{\Ealpha}{\mathbb{E}_{\alphab}}
\newcommand{\Ebeta}{\mathbb{E}_{\betab}}

\newcommand{\J}{\mathcal{J}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\tikzstyle{neuron}=[draw,circle,minimum size=20pt,inner sep=0pt, fill=white]
\tikzstyle{stateTransition}=[thick]
\tikzstyle{learned}=[text=red]

% KL-divergence double bar with: \infdiv{KL}{p}{q}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
#1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}[1]{D_{#1}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Latex settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{{./figs/}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Variational Noise-contrastive Estimation}
\author{Ben Rhodes}
\date{\today}						

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Abstract}
This thesis presents a method for estimating the parameters of unnormalised probability densities with unobserved (or `latent’) variables. The method is an extension of noise-contrastive estimation, which applies only to unnormalised, fully observed models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\textbf{An unnormalised model}, $\pnn(\u;\thetab)$, is a parametrised family of non-negative functions that you can think of as scaled probability density \footnote{unless stated otherwise, assume that all results apply equally to probability mass functions defined over discrete random variables, replacing integrals with sums as necessary.} functions that do not integrate to 1 for all values of $\thetab$. That is:
 \begin{align}
   \int_{\u }\pnn(\u;\thetab) \dif \u &= Z(\thetab) \neq 1 & \pnn(\u;\thetab) \geq 0
\label{eq:Zdef}
  \end{align}
where $Z(\thetab$) is called the \emph{partition function}. The partition function is important because it allows us to normalise $\pnn(\u;\thetab)$, obtaining a model $\pnorm(\u; \thetab)$ that, regardless of the value of $\thetab$, integrates to 1.
\begin{align}
  \pnorm(\u;\thetab)&=\frac{\pnn(\u;\thetab)}{Z(\thetab)}, & \int_{\u }\pnorm(\u;\thetab) \dif\u & = 1.
  \label{unnormalised density}
\end{align}
Unfortunately, the partition function is rarely computable analytically, and the cost of approximating it generally scales exponentially with the dimension of $\u$. The intractability of the partition function becomes problematic when we want to estimate good values of $\thetab$ given some data $\{\x_1, ..., \x_n\}$, where `good' --- in practical terms --- means that  $\pnorm(\x_i; \thetab)$ is large for all $i$ (whilst still satisfying the constraint of integrating to 1). It is problematic because the standard approach of maximum likelihood estimation requires us to maximise an objective that depends on the partition function; namely, the log-likelihood:
\begin{align}
\ell_n(\thetab) &= \sum_{i=0}^{n}  \log \pnorm(\x_i;\thetab) = \sum_{i=0}^{n}  \log \pnn(\x_i;\thetab) - n \log ( Z(\thetab) ),
\label{eq:log-likelihood}
\end{align}
To handle this intractability, multiple methods for estimating the parameters $\thetab$ without directly computing the partition function, such as pseudolikelihood \citep{besag1975statistical}, score matching \citep{hyvarinen2005estimation}, ratio matching \citep{hyvarinen2007some}, contrastive divergence \citep{hinton2006training}, persistent contrastive divergence \citep{younes1998stochastic, tieleman2009using} and noise-contrastive estimation (NCE) \citep{gutmann2012noise}. The final three methods --- contrastive divergence, its persistent variant, and NCE --- are perhaps the most versatile, being simple to implement and applicable to a large class of models. In some respects, NCE is preferable to contrastive divergence since it has a well-defined objective function (a point we shall return to later) and, under some quite general assumptions, is guaranteed to converge to the true value of $\thetab$. However, NCE cannot currently be applied to models with hidden, or \emph{latent}, variables, whereas contrastive divergence can. It is this deficiency of NCE that this thesis aims to remedy.

\textbf{A latent variable model} is a family of probability densities $\pnorm(\u; \thetab)$ for which we do not have a direct expression; instead we only have a joint $\pnorm(\u, \z; \thetab)$ density, where $\z$ are hidden variables, not present in our data set. We thus obtain $\pnorm(\u; \thetab)$ only after marginalisation:
\begin{align}
  \pnorm(\u; \thetab) = \int \pnorm(\u, \z;\thetab) \dif \z \ .
  \label{eq:marginal}
\end{align}

Learning the parameters $\thetab$ is problematic here for essentially the same reason as in unnormalised models: the presence of an intractable integral. In section \ref{sec:VNCE} we discuss a general strategy for overcoming this intractability using variational inference.

To see why latent variable models are important, it helps to consider two conceptually distinct (but mathematically equivalent) types of latent variables. The first type represent `missing data’; perhaps certain pixels in an image were corrupted, generating NaNs. The second type represent compressions of the visible data, which often (but not always) capture low-dimensional causes of the visible data. Such compressions are at the core of unsupervised learning, since the ability to build parsimonious models of the hidden causes of your observations appears to be a fundamental aspect of intelligence. Thus, it is primarily the second view of latent variables that motivates the widespread use of such models. 

Let us now consider the combined case of an unnormalised, latent variable model. To write down an expression for $\pnorm(\u; \thetab)$, we combine equations \ref{unnormalised density} and \ref{eq:marginal} to get:
\begin{align}
    \pnorm(\u; \thetab) = \frac{\int \pnn(\u, \z;\thetab) \dif \z}{Z(\thetab)} \ ,
\end{align}
where $Z(\thetab)$ is as we defined it in equation \ref{eq:Zdef}. This expression contains two troublesome integrals, and it is this double intractability that is the fundamental technical barrier we must overcome to perform parameter estimation.

\newpage
\section{Background}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                            MLE AND CD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Maximum Likelihood and Contrastive Divergence}
\label{sec:mle and cd}

As stated in the introduction, maximum likelihood learning consists of maximising the log-likelihood given in equation \ref{eq:log-likelihood}, which is a summation over $n$ data points. However, for much of the theory presented in this thesis, it will be convenient to remove the dependency on a finite set of data, and just refer to the underlying distributions that generate the data. To do this, note that the average log-likelihood is the sample version of:
\begin{align}
\label{eq:expectation log-like}
\ell(\thetab) &= \Edata \left[ \log \pnorm(\x;\thetab) \right] \\
              &= \Edata \left[\log \pnn(\x;\thetab) \right] - \log Z(\thetab) \\
              &= \Edata \left[ \log \int\pnn(\x, \z;\thetab) \dif \z \right] - \log \int\pnn(\x, \z;\thetab) \dif \x \dif \z \ ,
\end{align}
where $p^*$ is the data generating distribution. One can show that the gradient of $\ell(\thetab)$ is given by:
\begin{align}
    \label{eq:unnormalised latent var mle gradient}
    \gradtheta \ell(\thetab) = \Edata \E_{\z \sim p(\z | \x)} \left[ \gradtheta \log \pnn(\x, \z;\thetab) \right] - \E_{\x, \z \sim p(\x, \z)} \left[\gradtheta \log \left(\pnn(\x, \z;\thetab) \right) \right] \ ,
\end{align}
Intuitively, the first gradient `pushes up' on $\pnn$ wherever the \emph{data} is likely, whilst the second term `pushes down' on  $\pnn$ wherever the \emph{model} is likely. The net effect is to reallocate probability mass from regions where the data is unlikely to regions where it is likely. It is critical that reallocate mass, and not simply increase the total amount, since the model must integrate to one. At equilibrium, these two forces directly cancel, and the model equals the data generating distribution.


In general, the expectations in equation \ref{eq:unnormalised latent var mle gradient} will not be computable analytically. Then the question becomes, can we efficiently sample from the model's posterior over latents $p(\z | \x)$ and the from the joint $p(\x, \z)$? Sampling must be efficient since we need to re-sample after \emph{every} gradient update. For many models of interest, such as the Restricted Boltzmann Machine discussed in \ref{sec:rbm}, sampling from $p(\z | \x)$ is straightforward, whilst sampling from the joint $p(\x, \z)$ is problematic. The reason is that many unnormalised, latent variable models are defined via undirected graphs, which do not permit ancestral sampling and generally require us to use a Monte-Carlo Markov Chain (MCMC) method that can be very slow to converge.

Contrastive divergence \citep{hinton2006training} bypasses slow MCMC sampling by initialising the Markov chain at a data point and only running it for k-steps (rather than waiting for it to converge), where k is a tuning hyperparameter. Thus, the gradient updates are given by:
\begin{align}
    \label{eq:cd grad update}
    \nabla_{\thetab} \ell(\thetab) &= \Edata \E_{\z \sim p(\z | \x)} \left[ \nabla_{\thetab} \log \pnn(\x, \z;\thetab) \right] - \E_{\x, \z \sim p_k(\x, \z)} \left[\nabla_{\thetab} \log \left(\pnn(\x, \z;\thetab) \right) \right] \ , 
\end{align}
where $p_k$ is the distribution obtained after running the Markov chain for k steps, starting at a datapoint. \citet{bengio2009justifying} showed that the difference between the MLE gradient update and the contrastive divergence update is given by 
\begin{equation}
    \label{eq:cd bias}
    \E_{\x \sim p_k(\x)} \left[ \gradtheta \log p_k(\x) \right] \ .
\end{equation}
In addition, they showed that this term tends to zero as k tends to infinity and in the specific case of the RBM, provided a bound on this term. Non model-specific bounds seem hard to derive, and little is known in general about the convergence properties of contrastive divergence \citep{carreira2005contrastive}. One interesting result we do have is that the contrastive divergence update is \emph{not} the update of any objective function \citep{sutskever2010convergence}, which prevents us applying non-linear optimisation methods. Despite the weak theory supporting contrastive divergence, it has proven empirically to be an efficient learning technique in many situations.

A related learning method to contrastive divergence is  \emph{persistent} contrastive divergence \citep{younes1998stochastic, tieleman2009using}. Instead of initialising the Markov chains at a datapoint after every gradient update, we `persist' the state of the chain and reuse it across gradient steps. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                           INTRO TO NCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Noise-contrastive estimation}
\label{sec:nce}
NCE converts an unsupervised density estimation problem into a supervised classification problem, by training a (non-linear) logistic classifier to distinguish between the true data and samples from some reference distribution, which we refer to as noise samples.

Concretely, we first generate $m$ samples $\{\textbf{y}_1, \ldots \textbf{y}_m\}$ from a noise distribution $\pnoise(\textbf{y})$ and mix these samples with our data $(\textbf{x}_1, …., \textbf{x}_n)$. Our goal is now to classify whether a random sample $\u$ from this mixture came from the data-generating distribution $p$ or the noise $\pnoise$. We do so by calculating posterior probabilities, using our unnormalised model $\phi(\u; \thetab)$ as a surrogate for the true probability of the data $p(\u)$. If $\nu = \frac{m}{n}$, then:
\begin{align}
    \color{red}{\P( \u \sim \pdata; \thetab)} &= \color{red}{\frac{\pnn(\textbf{u}; \thetab)}{\pnn(\textbf{u}; \thetab) + \nu \pnoise(\textbf{u})}} \\
    \color{blue}{\P( \u \sim \pnoise; \thetab)} &= \color{blue}{\frac{\nu \pnoise(\textbf{u})}{\pnn(\textbf{u}; \thetab) + \nu \pnoise(\textbf{u})}.}
\end{align}
Note that $\P( \u \sim \pnoise; \thetab ) = 1 - \P( \u \sim \pdata; \thetab )$, which makes sense because this is a two-class classification problem, and so we have a Bernoulli distribution over the class label. Using the average log-likelihood for a Bernoulli, we derive the following objective function:
\begin{align}
  J_n(\thetab)&= 
  \frac{1}{n} \left\{ \color{red}{\sum_{i=1}^{n} \log  \P( \x_i \sim \pdata; \thetab)} 
    + \color{blue}{\sum_{i=1}^{m} \log\left[\P( \y_i \sim \pnoise; \thetab )\right]} \right\} \\
  & = \color{red}{\frac{1}{n} \sum_{i=1}^{n} \log \left(\frac{\pnn(\x_i; \thetab)}{\pnn(\x_i; \thetab) + \nu \pnoise(\x_i)}\right)}
    + \color{blue}{\frac{1}{n}\sum_{i=1}^{m} \log \left(\frac{\nu \pnoise(\y_i)}{\pnn(\y_i; \thetab) + \nu \pnoise(\y_i)}\right),} 
  \label{eq:Jn}
\end{align}
which is the sample version of $J(\thetab)$:
\begin{equation}
  J(\thetab) = 
   \color{red}{\Ex  \log \left(\frac{\pnn(\x; \thetab)}{\pnn(\x; \thetab) + \nu \pnoise(\x)}\right)}
  + \color{blue}{\nu \Ey \log \left( \frac{\nu \pnoise(\y)}{\pnn(\y; \thetab) + \nu \pnoise(\y)}\right).}
  \label{eq:J}
\end{equation}

If we denote the true data generating distribution by $\pnorm(\u; \theta*)$, and assume that this distribution is contained within our unnormalised family $\phi(\u;\thetab)$, then, under some rather general technical conditions described by \citet{gutmann2012noise}, optimising $J(\thetab)$ guarantees that $\thetab$ converges (in probability) to $\theta^*$ in the limit of infinite data.

Why should the \emph{normalised} data generating distribution live in an \emph{unnormalised} family? This assumption may seem odd at first, but note that it is simple to give the model family $\pnn(\u; \thetab)$ an extra degree of freedom by multiplying it with a scaling parameter $c$. This new parameter is estimated in conjunction with $\thetab$, so that we automatically obtain a normalised solution.\footnote{note, this is \emph{not} the same as normalising the model i.e.\ computing the partition function, which provides the normalisation constant for \emph{every} value of $\thetab$}

It turns out that for a large number of noise samples, $\nu$, the choice of noise distribution $q$ is not important. However, increasing $\nu$ uses more computational resource, so we still need to select an appropriate distribution. It remains an area of active research to select a noise distribution $\pnoise$ based on theoretically justified properties, with the main constraints currently being that it should be cheap to sample from, non-zero wherever $\pnn$ is non-zero, and similar to the underlying the data generating distribution.

In the case of latent variable models, the extended version of NCE that we present in this thesis is able to partially automate the construction of a suitable noise distribution, which is a significant advance over hand-crafted choices.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        INTRO TO VNCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Variational noise-contrastive estimation}
\label{sec:VNCE}
Guttmann [2018, unpublished] derived a \emph{variational} lower bound to the NCE objective function, which is the mathematical keystone to this thesis. Before presenting the bound, we first give a more standard introduction to the variational approach, which should make the subsequent material clearer.

\subsection{Variational inference and EM}
As discussed in section \ref{sec:mle and cd}, the expected average log-likelihood for a latent variable model is given by:
\begin{align}
    \ell(\thetab) &= \Ex  \log \left( \int \pnorm(\x, \z;\thetab) \dif \z  \right)  \\
                  &= \Ex \log \left( \Evar{\x} \left( \frac{\pnorm(\x, \z;\thetab)}{\q{\x}} \right)  \right)
\end{align}
where, in the second line, we applied a simple identity known as importance sampling, where the `auxiliary' density $q$ can be any valid density that is non-zero whenever $\pnorm$ is non-zero. We then apply Jensen's inequality, using the fact that the $\log$ function is concave, giving:
\begin{align}
    \ell(\thetab) &\geq \Ex \Evar{\x} \left( \log \left( \frac{\pnorm(\x, \z;\thetab)}{\q{\x}} \right) \right) \ .
    \label{eq:elbo}
\end{align}
It is straightforward to show that this inequality is tight when $\q{\x} = \pnorm(\z | \x; \thetab)$. This inequality directly leads to an iterative algorithm, known as Expectation Maximisation (EM) for estimating the parameters $\thetab$. Whilst the idea of EM is quite old, originally by \citet{dempster1977maximum}, the view presented here is similar to that given by \citet{neal1998view}. The algorithm simply involves alternating between optimising the right-hand side of equation \ref{eq:elbo} with respect to $\thetab$ (which we can do since we have an expression for $\pnorm(\x, \z;\thetab)$) and then, with our new $\thetab$, setting: $\q{\x} = \pnorm(\z | \x; \thetab)$. One can prove that this iterative algorithm always increases the log-likelihood.

EM, as stated above, assumes we have an expression for $\pnorm(\z | \x; \thetab)$. That is, we assume \emph{inference} of the latent variables is tractable. What if this is not the case? Well, we still want $q$ to approximate the posterior over latent variables, and so we could parametrise it with parameters $\alphab$. If this parametric family is sufficiently broad, we might hope that one of its members is close to the true posterior, and that we can find such a member by optimising $\alphab$. Thus, instead of resetting: $\q{\x} = \pnorm(\z | \x; \thetab)$, we optimise the right-hand side of \ref{eq:elbo} with respect to the \emph{variational parameters} $\alphab$.

How to perform this optimisation with respect to $\alphab$ is a matter of ongoing research. It is non-trivial because the right-hand side of $\ref{eq:elbo}$ involves an expectation with respect to $q(\z|\x; \alphab)$, so standard gradient-based methods are hard to straightforwardly apply. There are methods to cope with this obstacle, such as the reparameterisation trick \citep{kingma2015variational}, which we discuss in later chapters. 

\subsection{Variational lower bound for NCE}
Inspired by the variational lower bound for the log-likelihood, it's natural to wonder how we might apply a similar strategy to the NCE objective function in equation \ref{eq:J}. There are two fundamental steps: 
\begin{itemize}
    \item Use importance sampling to re-express the intractable integral over $\z$ as an expectation; call this E.
    \item Apply Jensen’s inequality to a concave function g(E).
\end{itemize}
One complication in applying such a strategy is that the intractable integral $\int \pnn(\u, \z;\thetab) \dif \z$ appears both inside the expectation with respect to the data:
\begin{align}
    \color{red}{\Ex  \log \left(\frac{\int \pnn(\x, \z;\thetab) \dif \z}{\int \pnn(\x, \z;\thetab) \dif \z + \nu \pnoise(\x)}\right)}
    \label{eq:J term1}
\end{align}
and it also appears in the second term, which is an expectation with respect to the noise distribution:
\begin{align}
    \color{blue}{\nu \Ey \log \left( \frac{\nu \pnoise(\y)}{\int \pnn(\y, \z;\thetab) \dif \z + \nu \pnoise(\y)}\right).}
    \label{eq:J term2}
\end{align}
Let us focus only on the first term (\ref{eq:J term1}). Introducing the notation:
\begin{equation}
  r(\x;\thetab) = \frac{\int \pnn(\x, \z;\thetab) \dif \z}{\pnoise(\x)} = \Evar{\x} \left( \frac{\pnn(\x, \z;\thetab)}{\q{\x}\pnoise(\x)} \right)
  \label{eq:r-def}
\end{equation}
and
\begin{equation}
  g(r) = -\log \left(1+\nu \frac{1}{r} \right),
\end{equation}
we see that:
\begin{align}
    \color{red}{\log \left(\frac{\int \pnn(\x, \z;\thetab) \dif \z}{\int \pnn(\x, \z;\thetab) \dif \z + \nu \pnoise(\x)}\right)} 
    &= \color{red}{- \log \left( 1 + \nu \frac{\pnoise(\x)}{\int \pnn(\x, \z;\thetab) \dif \z}\right)} \\
    &=   \color{red}{g(r(\x; \thetab)).}
\end{align}
It quickly follows from lemma \ref{lemma: f convex} (see appendix) that $g$ is a concave function of $r$. Since $r$ is really just an expectation, as is clear from \ref{eq:r-def}, we can apply Jensen's inequality:
\begin{align}
 \color{red}{g(r(\x; \thetab))}  
    &\geq  \color{red}{\Evar{\x} g \left( \frac{\pnn(\x, \z;\thetab)}{\q{\x}\pnoise(\x)} \right)} \\
    &\geq  \color{red}{-\Evar{\x} \log \left(1+\nu \frac{\q{\x}\pnoise(\x)}{\pnn(\x,\z; \thetab)}\right),} 
\end{align}
where, to get the last line, we simply substitute in the definition of $g$.

It does not appear possible to apply the same combination of importance sampling and Jensen's inequality to the second term of the NCE objective (\ref{eq:J term2}). However, we don’t necessarily need to. The intractable integral in the second term can be approximated just with importance sampling, \emph{re-using} the variational distribution $q$ that we got from the first term. Thus, we obtain a variational lower bound to the NCE objective function given by:
\begin{equation}
  \J_1(\thetab, q) 
  = \color{red}{-\Ex \E_{\z \sim q(\z|\x)} \log \left(1+\nu \frac{q(\z|\x)\pnoise(\x)}{\pnn(\x,\z; \thetab)}\right)}
   \color{blue}{ - \nu \Ey \log \left(1+\frac{1}{\nu} \E_{\z \sim q(\z | \y)} \left[\frac{\pnn(\y,\z; \thetab)} {q(\z | \y)\pnoise(\y)} \right]  \right).}
  \label{eq:obj1}
\end{equation}
We have that $J(\thetab) \ge \J_1(\thetab, q)$ for all $q$, and so we can apply variational EM, maximising $J(\thetab)$ by iterating between maximising $\J_1(\thetab,q)$ with respect to $\thetab$, and the tightening of the bound by maximising $\J_1(\thetab,q)$ with respect to the variational distribution $q$.


\subsection{Necessary ingredients to apply VNCE}
\label{sec:optimisation of var objective}
Having established the objective function \ref{eq:obj1} for VNCE, it is helpful to explicitly lay out the ingredients required for optimising it. We require:
\begin{itemize}
    \item A noise distribution over the visible variables $\pnoise$. This density should, ideally, be cheap to evaluate and sample from, and similar to the data distribution.
    \item A variational distribution over latent variables $q$. This will typically have its own parameters $\alphab$, apart from the case where we set $q$ to equal the model's posterior over latents. This is only possible for models with tractable posteriors. 
    \item If the variational distribution $q$ has parameters $\alphab$, then we need compute $\nabla_{\alphab} J_1(\alphab)$. This is difficult in general, but depending on the parameterisation of $q$, it may be possible to do analytically, perhaps using the reparameterisation trick \citep{kingma2015variational}.
    \item The gradient $\nabla_{\thetab} J_1(\thetab)$. A derivation of this gradient is given in the appendix, section \ref{sec:appendix grad of J1}. The key term required to compute it is the gradient of the logarithm of the unnormalised model: $\nabla_{\thetab}(\log \phi(\u; \thetab))$.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        RESEARCH OBJECTIVES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Research Objectives}
\label{sec:research objectives}
In the previous section, we established a method of parameter estimation for unnormalised, latent variable models involving the maximisation of a variational lower bound to the NCE objective function, as given in equation \ref{eq:obj1}. We will refer to this method as VNCE. The over-arching goal in the remainder of the thesis is to appraise the usefulness of VNCE across different models, data sets, and evaluation metrics. In addition, we aim to contribute a better theoretical understanding of the new estimation method, which may involve studying its asymptotic properties or its mathematical similarities with other methods such as contrastive divergence.

\subsection{Models and data sets}
The first steps, which are ongoing, involve applying the estimator to models of different levels of complexity with synthetic data. To this end, we have investigated a simple unnormalised mixture of two gaussians, discussed in section \ref{sec:mog}, and a more sophisticated model: the Restricted Boltzmann Machine (RBM), discussed in section \ref{sec:rbm}. The RBM has received a significant amount of attention over the last 15 years in the machine learning community, and there is thus a wealth of published material on estimating its parameters. A natural first step is to therefore build on this work.

After experimenting on synthetic data, we will then apply an RBM to real data. Some of the most thorough empirical work to date on different methods for learning RBMs \citep{marlin2010inductive} considered a range of data sets such as MNIST\footnote{http://yann.lecun.com/exdb/mnist}, the small 20-Newsgroups data set\footnote{http://www.cs.toronto.edu/roweis/data.html} and an image dataset from CalTech101\footnote{http://www.vision.caltech.edu/Image\_Datasets}. It makes sense to evaluate the new estimation method on the same data sets, to enable straightforward comparisons of my results with the published literature. That said, one important finding by \citet{marlin2010inductive} was that comparisons of estimation methods does vary significantly depending on the data set, and so I will consider experimenting with an even greater variety of data later in the thesis, assuming the preliminary results warrant further investigation.

\subsection{Evaluation metrics}
There are many possible metrics to consider. Of particular interest are the quality of the probabilistic model (\emph{does it assign high probability to the test data?}), the asymptotic properties of the estimator (\emph{does it use data efficiently?}) and the computational cost of the estimator (\emph{does it converge quickly?}).

With regards to the quality of the probabilistic model, we want to know: is it close to the true data generating distribution? One way of measuring closeness between distributions is the KL divergence. We can estimate the KL divergence between the true distribution and our model by evaluating the model's log-likelihood on the test data. For relatively simple, low-dimensional models, where we can analytically solve for the partition function and marginal distribution over observed variables, we can indeed use the log-likelihood as an evaluation metric. For more complex models, we must resort to approximations of the log-likelihood.

With regards to asymptotic properties, it helps to consider the case of ordinary NCE. There it was proven that the estimator converges (in distribution) to a normal distribution centred on the true parameter value, in the limit of infinite data. It is natural to wonder if a similar result might hold in our case. That is, do we have:
\begin{align}
    \sqrt{n} (\thetaHat_n - \thetab) \rightarrow \mathcal{N}(0, V)
\end{align}
In the 1-dimensional case, $V/n$ is a scalar called the asymptotic variance. We would like this value to be small, as this implies that the estimator uses data efficiently when approximating the true parameter value.

Another means of evaluation is through comparison with the ordinary NCE objective function, which is an upper bound to our new objective function. This is only possible for models where we can analytically integrate out the latent variables, but this restriction isn't as great as it first seems, because RBMs with only a small number of hidden units - in the single digits - can still be useful generative models of real data.

A weaker, but still interesting measure of the usefulness of a density model is to apply it to discriminative tasks. For instance, having trained an RBM to capture the statistics of patches of natural images, you can then use that generative model as part of image-based classifier. This form of measurement is quite indirect, but still potentially useful. 

Finally, for all the above metrics, we can ask: what is the computational cost of achieving a particular score under that metric? This is an important practical consideration, and given that NCE is relatively fast compared to other estimators of unnormalised models, we might hope the same is true of VNCE.

\subsection{Baselines}
\label{sec: baselines}
Now that we have a sense of the models, data sets and evaluation metrics available, we have one final important task: specifying a baseline estimation method. As mentioned, for simple models, we can actually evaluate the intractable integrals that normally prevent maximum likelihood and NCE from being applied, and so we can compare the new method to these. For more complex methods, such as RBMs with many hidden units, the dominant approach to learning is to use contrastive divergence (or some variant, such as persistent contrastive divergence). Hence, the primary research questions underlying this thesis are: 

\vspace{2mm}
\noindent \emph{How does VNCE compare to contrastive divergence under the metrics specified above? Is one method more theoretically grounded than the other, and in what sense?}

\subsection{Additional Research objectives}
You can think of the research objectives above as a road-map for performing a thorough empirical and theoretical comparison of VNCE to others, notably contrastive divergence. But the new method is actually more general than contrastive divergence, in two senses.

Firstly, we are able not only to estimate the parameters of a model, but also an approximation of the associated normalising constant \footnote{Note, we only learn the normalising constant for the final parameters learned after convergence, not intermediate values.}. Previously, this had to be approximated separately with a method such as annealed importance sampling \citep{salakhutdinov2008quantitative}, and so it would be interesting to compare these approximations.

Secondly, and more significantly, contrastive divergence requires you to be able to sample from the model's posterior over latent variables. VNCE does not require this; it only requires an approximation of the posterior, which is learned automatically. Hence, there are interesting models, such as an RBM with connections between the hidden variables, or stacked RBMs, where contrastive divergence is much harder to apply than ours. This potentially opens a new class of models for which learning was previously intractable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                           RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Results}
\label{sec:results}
In section \ref{sec:VNCE}, we presented a variational lower bound to the NCE objective function, $J_1(\thetab, q)$, that depends on an auxiliary distribution $q$ that we optimise in conjunction with the model's parameters $\thetab$. We would like to have a better theoretical understanding of how this optimisation procedure relates to ordinary NCE and also to standard variational inference with the log-likelihood. In particular, is the maximum of $J_1$ with respect to $\thetab$ and $q$ equal to the maximum of the NCE objective, $J$, with respect to $\thetab$? And what form does the optimal $q$ take? 

We obtain a positive answer to the first question, showing that VNCE has the same maximum as NCE, and in doing so derive the optimal $q$ to be the model's posterior over latents $p(\z | \u ; \thetab)$. This is the same optimal $q$ as in standard variational inference, and implies that VNCE is not only useful for parameter estimation, but also performs approximate inference of the latent variables. As discussed in section \ref{sec:VNCE}, standard variational inference minimises the KL-divergence between the approximate and true posterior. In contrast, we show that VNCE minimises a different \emph{f-divergence} between the two posteriors. This is likely to have important consequences on the type of approximate posterior we learn whenever $q$ is not flexible enough to match the true posterior. We discuss these consequences in more detail after deriving the necessary equations. Before that however, we need to make the idea of an f-divergence precise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                           EM NON-DECREASING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\begin{definition}
An \emph{f-divergence} $\infdiv{f}{p}{q}$ between two probability densities $p$ and $q$, is defined as
\begin{equation}
    \infdiv{f}{p}{q} = \mathbb{E}_{u \sim q} \left[ f\left(\frac{p(u)}{q(u)}\right) \right],
\end{equation}
where $f$ is a convex function satisfying $f(1) = 0$.
\end{definition}
It is a straight-forward application of Jensen's inequality to show that f-divergences are non-negative:
\begin{equation}
    \infdiv{f}{p(u)}{q(u)} = \mathbb{E}_q \left[ f\left( \frac{p(u)}{q(u)} \right) \right] \geq f\left( \mathbb{E}_q \left[  \frac{p(u)}{q(u)} \right]  \right) = f \left( \int p(u) \dif u \right) = f(1) = 0
\end{equation}
and obtain their minimum when $p = q$. These two properties make f-divergences useful for learning a data generating distribution $p$ by optimising the parameters of $q$ to minimise $\infdiv{f}{p(u)}{q(u)}$. In fact, maximum likelihood learning minimises the KL divergence, which is one example of an f-divergence where $f(u) = u \log(u)$.
\begin{lemma}
    The difference between the NCE and VNCE objective functions is equal to the expectation of an f-divergence between the true and approximate posterior. Specifically,
    \begin{equation}
        J(\thetab) - J_1(\thetab, q) = \Ex \left[ \infdiv{f_{\x}}{p(\z | \x; \thetab)}{q(\z | \x)} \right],
    \end{equation}
where
\begin{equation}
    f_{\x}(u) = \log(v_{\x} + (1 - v_{\x})u^{-1}), \hspace{5mm} v_{\x} = \frac{\pnn(\x; \thetab)}{\pnn(\x; \thetab) + \nu p_{y}(\x)}
\end{equation}
Moreover, this f-divergence can be expressed as the difference of two KL-divergences:
\begin{equation}
\label{eq: f-divergence is sum of two KLs}
     \infdiv{f_{\x}}{p(\z | \x; \thetab)}{q(\z | \x)} = \infdiv{KL}{q(\z | \x)}{p(\z | \x; \thetab)} - \infdiv{KL}{q(\z | \x)}{m(\z, \x ; \thetab)},
\end{equation}
where $m(\z, \x ; \thetab) = v_{\x}p(\z | \x; \thetab) + (1 - v_{\x})q(\z | \x)$ is a convex combination of the true and approximate posteriors.
\end{lemma}
\begin{proof}
Recall that the NCE objective $J$ (\ref{eq:J}) and the VNCE objective $J_1$ (\ref{eq:obj1}) consist of two terms: the first is an expectation with respect to the data, the second an expectation with respect to the noise distribution $\pnoise$. The second terms of $J$ and $J_1$ are identical, so their difference equals the difference between their first terms:
\begin{align}
    J(\thetab) - J_1(\thetab, q) 
    &= \Ex \log \left( \frac{\pnn(\x)}{\pnn(\x) + \nu p_{y}(\x)} \right) 
    - \Ex \Evar{x} \log \left( \frac{\pnn(\x, \z)}{\pnn(\x, \z) + \nu q(\z | \x) p_{y}(\x)} \right) \\
    &= \Ex \Evar{x} \left[ \log \left( \frac{\pnn(\x)}{\pnn(\x) + \nu p_{y}(\x)} \right) + \log \left( 1 + \frac{q(\z | \x) p_{y}(\x)}{\phi(\x) p(\z | \x)} \right) \right] \\
    &= \Ex \Evar{x} \left[ \log \left( \frac{\pnn(\x)}{\pnn(\x) + \nu p_{y}(\x)} + \frac{ \pnn(\x) p_{y}(\x)}{(\pnn(\x) + \nu p_{y}(\x))\phi(\x)} \frac{q(\z | \x)}{p(\z | \x)} \right) \right] \\
    \label{eq: f divergence formula}
    &= \Ex \Evar{x} \left[ \log \left( v_{\x} + (1 - v_{\x}) \frac{q(\z | \x)}{p(\z | \x)} \right) \right] \\
    &=  \Ex \left[ \infdiv{f_{\x}}{p(\z | \x; \thetab)}{q(\z | \x)} \right],
\end{align}
where $f_{\x}(u) = \log(v_{\x} + (1 - v_{\x})u^{-1})$. To ensure that that $D_{f_{\x}}$ is a valid f-divergence, we need to prove that $f$ is convex and $f_{\x}(1) = 0$. The latter is trivial, since $f_{\x}(1) = \log(v_{\x} + (1 - v_{\x})) = \log(1) = 0$, and convexity follows directly from lemma \ref{lemma: f convex} in the appendix.

We now prove that this f-divergence can be expressed as the difference of two KL-divergences as in equation \ref{eq: f-divergence is sum of two KLs}. To do this, we pull $q/p$ outside of the log in equation \ref{eq: f divergence formula}:
\begin{align}
    \infdiv{f_{\x}}{p(\z | \x; \thetab)}{q(\z | \x)} 
    &= \Evar{x} \left[ \log( \frac{q(\z | \x)}{p(\z | \x)}) \right] 
    + \Evar{x} \left[ \log \left( v_{\x}\frac{p(\z | \x)}{q(\z | \x)} + (1 - v_{\x}) \right) \right] \\
    &= \Evar{x} \left[ \log( \frac{q(\z | \x)}{p(\z | \x)}) \right] 
    - \Evar{x} \left[ \log \left( \frac{q(\z | \x)}{v_{\x}p(\z | \x) + (1 - v_{\x})q(\z | \x)} \right) \right] \\
    &= \infdiv{KL}{q(\z | \x)}{p(\z | \x; \thetab)} - \infdiv{KL}{q(\z | \x)}{m(\z, \x ; \thetab)}.
\end{align}
where $m(\z, \x ; \thetab) = v_{\x}p(\z | \x; \thetab) + (1 - v_{\x})q(\z | \x)$.
\end{proof}
\begin{theorem}

\end{theorem}
The above results lead directly to an expectation described in section \ref{sec:VNCE}, which states that it never decreases the log-likelihood.
\begin{corollary}
    Given a random initial starting point $\thetab_0$, and the following optimisation procedure:
    \begin{enumerate}
        \item (E-step) $q_k(\z | \u) = p(\z | \u ; \thetab_k) $ 
        \item (M-step) $\thetab_{k+1} = \argmax{\thetab} J_1(\thetab, q_k) $ 
        \item Unless converged, repeat steps 1 and 2
    \end{enumerate}
it follows that, for all $k \in \mathbb{N}$, we have:  $J(\thetab_{k+1}) \geq J(\thetab_k)$, where $J$ is the NCE objective function.
\end{corollary}
\begin{proof}
    Let $k \in \mathbb{N}$. In section \ref{sec:VNCE}, we showed that that the NCE objective is lower bounded by the VNCE objective, so that: $J(\thetab_k) \geq J_1(\thetab_k, q)$ for all $q$. We derived this by applying Jensen's inequality to the first term of the NCE objective function. Specifically, we showed that:
    \begin{align}
        \label{eq: jensen inequality inside theorem}
       \log \left(\frac{\pnn(\x; \thetab_k)}{\pnn(\x; \thetab_k) + \nu \pnoise(\x)}\right) 
       &\geq -\Evar{\x} \log \left(1+\nu \frac{\q{\x}\pnoise(\x)}{\pnn(\x,\z; \thetab_k)}\right) \ .
    \end{align}
    We now want to show that the above inequality becomes an \emph{equality} after the E-step of optimisation. To see this, first note that we can rewrite $\pnn$ as:
    \begin{equation}
        \pnn(\x, \z; \thetab_k) = p(\z | \x; \thetab_k) \pnn(\x ; \thetab_k) \ , 
    \end{equation}
    since if we divide both sides by $Z(\thetab_k)$, then the equation just becomes a straightforward factorisation of the joint density. Therefore, during the E-step of optimisation when we set $q_k$ such that $q_k(\z | \x) = p(\z | \x ; \thetab_k)$, it follows that:
    \begin{align}
        -\E_{\z \sim q_k(\z | \x)} \log \left(1+\nu \frac{q_k(\z | \x)\pnoise(\x)}{\pnn(\x,\z; \thetab_k)}\right)
        &= -\E_{\z \sim p(\z | \x ; \thetab_k)} \log \left(1+\nu \frac{\pnoise(\x)}{\pnn(\x ; \thetab_k)}\right) \\
        &= \log \left(\frac{\pnn(\x; \thetab_k)}{\pnn(\x; \thetab_k) + \nu \pnoise(\x)}\right) \ ,
    \end{align}
    where, to reach the final line, we simply dropped the expectation since the expression inside the expectation was constant with respect to $\z$, and rearranged the fraction inside the log. This proves that we have equality in equation \ref{eq: jensen inequality inside theorem}, and hence $J_1(\thetab_k, q_k) = J(\thetab_k)$.

    Now, in the M-step of optimisation, we have
    \begin{equation}
        \thetab_{k+1} = \argmax{\thetab} J_1(\thetab, q_k) \ \ \Longrightarrow \ \  J_1(\thetab_{k+1}, q_k) \geq J_1(\thetab_k, q_k) \ , 
    \end{equation}
    We therefore obtain:
    \begin{equation}
        J(\thetab_{k+1}) \geq J_1(\thetab_{k+1}, q_k)  \geq J_1(\thetab_k, q_k) = J(\thetab_k) \ .
    \end{equation}
\end{proof}
Observe that, just as in the ordinary EM algorithm, the above result does not hold in general if we only take a `partial' E-step, by making $q$ close, but not exactly equal, to $p_{\z | \x}$. However, we can always take a partial M-step, increasing the value of $J_1(\thetab, q_k)$ through a few gradient steps without computing the true argmax.


\subsection{Mixture of Gaussian model}
\label{sec:mog}
As a simple illustration of the correctness of VNCE, we apply it to a toy mixture of two Gaussians model, where the only unknown parameter is the standard deviation of one of the Gaussians. We consider two cases: firstly a normalised MoG, and then an unnormalised version. Finally, for the unnormalised version, we run a population analysis to compare the performance of VNCE against both NCE and maximum likelihood estimation across multiple sample sizes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        NORMALISED MIXTURE OF GAUSSIANS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The normalised case}

%\begin{figure}[ht]
%  \centering
%  \includegraphics[scale=0.4]{em-loss-curve-mog.pdf}
%    \caption{\label{fig:mog-em-loss-curve} EM type algorithm applied to a normalised Mixture of Gaussians %model. The red lines indicate when we make the update $q(z|u) = p(z|u; \theta_k)$ where $\theta_k$ is the %value of $\theta$ at that point during the optimisation.}.
%\end{figure}

\noindent A common latent variable model is the Mixture of Gaussian (MoG) model. For a mixture with two components, a single binary latent variable $z \sim \mathcal{B}er(\pi)$ determines which of two Gaussians the data was generated from. if $\pi = 1/2$, we have the following joint:
\begin{equation}
\pnorm(u, z; \theta) = \frac{(1-z)}{2} \mathcal{N}(u; 0, \theta) + \frac{z}{2} \mathcal{N}(u; 0, \sigma_1).
\label{eq:mog standard}
\end{equation}
We assume that the variance of the second component, $\sigma_1^2$, is known, and our task is to estimate the value of $\theta$. Whilst this estimation problem does not involve an unnormalised model, we can still apply VNCE to it. This provides a useful sanity check of the method's validity in a simple setting where we do not have a scaling parameter to estimate in conjunction with $\theta$.

For a simple experiment, we set $\sigma_1 = 1$ and let $\theta^* = 4$ be the true value of $\theta$. We then make the following choices of noise and variational distribution:
\begin{align}
    \label{eq:normalised mog posterior}
    \pnoise(u) &= \mathcal{N}(u; 0, \theta^*) \\
    q(z|u) &= p(z=0 \ | \ u; \theta) = \left(  1 + \frac{\theta_k}{\sigma_1} \exp \left(\frac{- u^2}{2} \left( \frac{1}{\sigma_1^2} - \frac{1}{\theta^2} \right) \right) \right)^{-1}.
\end{align}
The choice of noise distribution is not too important for this simple model, so long as it very approximately matches the data generating distribution. With this setup, we can then apply the EM type algorithm discussed in section \ref{sec:optimisation of var objective}. We can visualise this algorithm by iteratively plotting the variational bound, for all values of $\theta$, each time we update the variational distribution $q$ in figure \ref{fig:normalised-mog}. It is clear from the figure that the algorithm converges to the correct value of $\theta$.

\afterpage{
    \thispagestyle{empty}
    \begin{figure}[ht]
      \centering
      \caption{\label{fig:normalised-mog} EM type algorithm applied to a normalised Mixture of Gaussians model. The notation $J^1_{\theta_k}$ in the legends refers to the variational lower bound of the NCE objective function where the variational distribution $q(z|u)$ is set to $p(z|u; \theta_k)$}.
      \includegraphics[width = 0.9 \textwidth]{variational-em.pdf}
    \end{figure}
    \clearpage
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        UNNORMALISED MIXTURE OF GAUSSIANS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The unnormalised case}

\noindent We would like to modify the MoG given in equation \ref{eq:mog standard} to obtain an unnormalised family. An obvious candidate is:
\begin{equation}
\phi(u, z; \theta, c) = e^{-c} \left( (1-z) e^{-\frac{u^2}{2 \theta^2}} + z e^{-\frac{u^2}{2 \sigma_1^2}} \right)
\label{eq:mog unnormalised}
\end{equation}
where $e^{-c}$ is a scaling parameter needed for NCE, as discussed at the end of section \ref{sec:nce}. It is important to note that the usual MoG family in equation \ref{eq:mog standard} is \emph{not} nested within unnormalised family in equation \ref{eq:mog unnormalised}. We can see this more easily by normalising \ref{eq:mog unnormalised}, giving us:
\begin{equation}
    \pnorm(u, z, \theta) = (1-z)\frac{\theta}{\theta + \sigma_1} \mathcal{N}(u; 0, \theta) +
                     z\frac{\sigma_1}{\theta + \sigma_1} \mathcal{N}(u; 0, \sigma_1)
\label{eq:mog normalised version of unnormalised}
\end{equation}
We see that the parameter $\theta$ of the unnormalised MoG controls both the width of the first component and its scale relative to the second component. 

As before, fix $\sigma_1 = 1$ and $\theta^*=4$. Using equation \ref{eq:mog normalised version of unnormalised}, we can easily sample synthetic data from our unnormalised model $\phi$ (still under the assumption that $z \sim \mathcal{B}er(\frac{1}{2})$). To do so, we simply sample $w \sim \mathcal{B}er(\frac{\sigma_1}{\theta + \sigma_1})$ and then sample $\{x_1, ..., x_i\}$ data points:
\[
    x_i \sim \left\{\begin{array}{lr}
        \mathcal{N}(u; 0, \theta), & \text{if } w = 0\\
         \mathcal{N}(u; 0, \sigma_1), & \text{if } w = 1
        \end{array}
\]
Using this synthetic data, we can proceed as before, using an EM algorithm to learn the parameter $\theta$. To do this, we would need to use the posterior over latents, given by:
\begin{equation}
    p(z=0 \ | \ u) = \frac{1}{1 + \exp(\frac{-u^2}{2} (\frac{1}{\sigma_1^2} + \frac{1}{\sigma_0^2}) )}.
\end{equation}
However, for more complex models, we do not have access to such a posterior, and so it is important that VNCE still works when we approximate it with a parametrised variational distribution $q$. To test this, we use the following $q$:
\begin{equation}
    q(z=0 \ | \ u; \textbf{w}) = \frac{1}{1 + \exp(w_0 + w_1u + w_2u^2)}, 
\end{equation}
which clearly contains the true posterior.

Visualising learning is less straightforward now that our parameter set is 2-dimensional, but we can compare the contour plot of the true NCE objective function against the lower bound obtained at the end of learning. This is given in figure \ref{fig:unnormalised mog contour}. We see that the algorithm converges to the correct value of $\theta$, and correct normalising constant.

\afterpage{
    \thispagestyle{empty}
    \begin{figure}[ht]
    \centering
    \includegraphics[width = 0.8 \textwidth]{unnormalised-mog-countour.pdf}
    \vspace*{-18mm}
    \caption{\label{fig:unnormalised mog contour} Contour plot of true NCE objective function compared to the variational lower bound for an unnormalised Mixture of Gaussians. For the lower bound, we have set the variational distribution $q(z|u)$ to be the posterior $p(z|u; \tilde{\theta})$, where $\tilde{\theta}$ is our final estimate obtained after optimisation. The vertical axis corresponds to $c$, the scaling parameter.}
    \end{figure}
    \clearpage
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                       MOG POPULATION ANALYSIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tpb]
\vspace{-15mm}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{sample-size-against-mse.pdf}
  \caption{Standard deviation}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{sample-size-against-mse-scaling-param.pdf}
  \caption{Scaling parameter}
  \label{fig:sub2}
\end{subfigure}
\caption{log sample size against log mean-squared error (MSE) produced when estimating the standard deviation and scaling parameter for 500 different unnormalised Mixture of Gaussian models. The thick central lines show the median MSE of the 500 runs, whilst the dashed lines mark the 1st and 9th deciles. The constant negative slope of the red line is evidence of the consistency of VNCE.}
\label{fig:mse-mog}
\end{figure}
\subsubsection{Population Analysis of unnormalised Mixture of Gaussian model}
Figure \ref{fig:mse-mog} shows the MSE $\mathbb{E} ||\theta - \theta^*||^2$ for VNCE, NCE and maximum likelihood across multiple runs and with different sample sizes. To produce it, we generated 500 distinct ground-truth values for standard deviation parameter in the unnormalised MoG, sampling uniformly from the interval $[2, 6]$. For each of the 500 $\theta^*$s, we estimate it using all three estimation methods and with a range of sample sizes. Every run was initialised from five random values and the best result out of the five was kept in order to avoid local optima, which exist since both the likelihood and NCE objective functions are bi-modal.

Figure \ref{fig:mse-mog} (a) demonstrates that the estimation accuracy of VNCE increases with sample size, and is comparable to that of NCE. This gives some evidence of the consistency of VNCE. We note that the comparison is complicated by the fact that NCE was particularly prone to falling into local optima (despite our use of multiple random initialisations). This is why NCE's worst-case accuracy is significantly worse than that of the other two methods, as shown by the upper dashed blue line.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        RESTRICTED BOLTZMANN MACHINES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Restricted Boltzmann Machine}
\label{sec:rbm}

The Restricted Boltzmann Machine (RBM) \citep{smolensky1986information} is an unnormalised, latent variable model over binary vectors given by:
\begin{align}
p(\u, \z, ; \thetab)  \propto \exp(\u^TW\z + \textbf{a}^{T}\u + \textbf{b}^T\z) \ ,
\label{eq:rbm}
\end{align}
where $W \in \mathbb{R}^{d \times m}$; $\textbf{a}, \u \in \mathbb{R}^{d}$; $\textbf{b}, \z \in \mathbb{R}^{m}$, and $\thetab \equiv (W, \textbf{a}, \textbf{b})$. We refer to $W$ as the weight matrix and $\textbf{a}$ and $\textbf{b}$ as the biases. $\u$ contains the visible variables/units and $\z$ the hidden units, which are typically of lower dimension since we would like to find a smaller set of random variables that capture the correlations between the higher-dimensional observations.

\begin{figure}[!h]
\centering
% Adapted from: https://github.com/MartinThoma/LaTeX-examples/blob/master/tikz/restricted-boltzmann-machine/restricted-botzmann-machine.tex
\begin{tikzpicture}[scale=2]
    % \draw ;
    \draw[fill=black!30, rounded corners] (-0.2, -0.2) rectangle (3.2, 0.2) {};
    \draw[fill=black!30, rounded corners] (0.3, 0.8) rectangle (2.7, 1.2) {};

    \node (u1)[neuron] at (0, 0) {$u_1$};
    \node (u2)[neuron] at (1, 0) {$u_2$};
    \node (u3)[neuron] at (2, 0) {$u_3$};
    \node (u4)[neuron] at (3, 0) {$u_4$};
    \node[right=0.1cm of u4] (u) {$\textbf{u} \in \{0, 1\}^4$};
    \node[learned,below=0.1cm of u1] (a1) {$a_{1}$};
    \node[learned,below=0.1cm of u2] (a2) {$a_{2}$};
    \node[learned,below=0.1cm of u3] (a3) {$a_{3}$};
    \node[learned,below=0.1cm of u4] (a4) {$a_{4}$};

    \node (z1)[neuron] at (0.5, 1) {$z_1$};
    \node (z2)[neuron] at (1.5, 1) {$z_2$};
    \node (z3)[neuron] at (2.5, 1) {$z_3$};
    \node[right=0.1cm of z3] (z) {$\textbf{z} \in \{0, 1\}^3$};
    \node[learned,above=0.1cm of z1] (b1) {$b_{1}$};
    \node[learned,above=0.1cm of z2] (b2) {$b_{2}$};
    \node[learned,above=0.1cm of z3] (b3) {$b_{3}$};

    \node[learned] (W) at (3.5, 0.5) {$W \in \mathbb{R}^{3 \times 4}$};

    \draw[learned,stateTransition] (u1) -- (z1) node [midway,above=-0.06cm,sloped] {$w_{1,1}$};
    \draw[stateTransition] (u1) -- (z2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u1) -- (z3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (u2) -- (z1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u2) -- (z2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u2) -- (z3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (u3) -- (z1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u3) -- (z2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u3) -- (z3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (u4) -- (z1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (u4) -- (z2) node [midway,above=-0.06cm,sloped] {};
    \draw[learned,stateTransition] (u4) -- (z3) node [midway,above=-0.06cm,sloped] {$w_{4,3}$};

\end{tikzpicture}
\caption{The undirected graph for a Restricted Boltzmann Machine.}
\label{fig:rbm-graph}
\end{figure}

In order to see how the hidden units in an RBM explain the couplings between the visible units, it helps to picture an undirected graph over which the model factorizes as in figure \ref{fig:rbm-graph}. Note that the Markov blanket of each $u_i$ is $\z$, and, conversely, the Markov blanket of each $z_i$ is $\u$. Therefore, we obtain the factorisations:
\begin{equation}
    p(\u | \z) = \prod_{i=1}^d p(u_i | \z)  \ \ \ \ \ \ \ p(\z | \u) = \prod_{i=1}^m p(z_i | \u) \ ;
    \label{eq: rbm conditional indpendicies}
\end{equation}
the first factorisation says that the visible units are independent given the latents and thus the latents capture correlations in the visibles.

We gain further insight into the model's properties by not only considering it's conditional independencies, but its specific parametric form, as given in equation \ref{eq:rbm}. If we marginalise out the latent variables, we obtain:
\begin{align}
    p(\u) &= \sum_{z_1, ..., z_m} p(\u, \z) \\
          &\propto \exp(\textbf{a}^T\u) \prod_{i=1}^m \sum_{z_i \in \{0, 1\}} \exp(b_i z_i + z_i \u^T W_{:, i}) \\
          &\propto \exp(\textbf{a}^T\u) \prod_{i=1}^m (1 + \exp(b_i + \u^T W_{:, i})) \ ,
          \label{eq:marginal of rbm}
\end{align}
where $W_{:, i}$ is the $i^{th}$ column of the weight matrix $W$. Since $p(\u)$ is a product over $m+1$ terms, we can think of the RBM as a `product of experts' \citep{hinton2006training}:
\begin{equation}
    p(\u) \propto \prod_{i=1}^{m+1} f_i(\u)
\end{equation}
where each expert `votes' on the probability of a vector $\u$. Since the experts act multiplicatively, the resulting distribution can be quite sharp, since each expert can vote for a specific sub-region of the input space. In the RBM, each expert takes the inner product of a fixed `template' vector --- $W_{:,i}$ or $\textbf{a}$ --- with the input $\u$, and so inputs that match multiple templates are likely under the model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                         TRAINING RBMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training Restricted Boltzmann Machines}
\label{sec:training_rbms}

The factorisation of p(\u) in equation \ref{eq:marginal of rbm} shows that marginalising out the latents is cheap for an RBM, costing only $O(m)$ time, where $m$ is the dimension of the latent vector $\z$. However, computing the partition function \emph{is} intractable, since we would have to compute:
\begin{align}
    Z(\thetab) = \sum_{\u \in \{0, 1\}^d} \exp(\textbf{a}^T\u) \prod_{i=1}^m (1 + \exp(b_i + \u^T W_{:, i})) \ ,
\end{align}
which cannot be further factorised due to the presence of $\u$ in every term of the product. Naively, computing $Z(\thetab)$ costs $O(m2^d)$. We can rearrange the sums to reduce this to $O(d2^m)$, but this is still exponential in the number of hidden units. Further optimisations exist to reduce the cost, but it is still infeasible to compute for much more than around 30 hidden units. 

Hence, we are not dealing with the general case that VNCE was designed for: when \emph{both} marginalising out the hiddens and computing the partition function are intractable. Since only the latter is intractable, we can apply NCE to the problem. As with the mixture of Gaussians, this is useful since we can compare the performance of NCE to VNCE.

Another important comparison to make is with contrastive divergence since it is perhaps the most common method for training an RBM. As discussed in section \ref{sec:mle and cd}, contrastive divergence roughly approximates the gradient ascent rule for maximum likelihood learning, using updates of the form:
\begin{align}
    \label{eq:cd grad update for rbm}
    \nabla_{W} \ell(\thetab) &= \Edata \E_{\z \sim p(\z | \x)} \left[ \nabla_{W} \log \pnn(\x, \z;\thetab) \right] - \E_{\x, \z \sim p_k(\x, \z)} \left[\nabla_{W} \log \left(\pnn(\x, \z;\thetab) \right) \right] \\
                      &= \Edata \E_{\z \sim p(\z | \x)} \left[ u_i z_j \right] - \E_{\x, \z \sim p_k(\x, \z)} \left[ u_i z_j \right] \ ,
\end{align}
for the weight matrix $W$ (similar updates are used for the biases). The notation $p_k$ refers to the distribution obtained after running a Markov Chain Monte Carlo method for $k$ steps, starting at the data generating distribution $p^*$. If we took $k \rightarrow \infty$, then we would obtain $p$ (the current model). Whilst we are free to choose amy MCMC method, in the case of an RBM, the conditional independence statements in equation \ref{eq: rbm conditional indpendicies} allow us to implement efficient block-Gibbs sampling, where we alternate between sampling $\u | \z$ and $\z | \u$.

To our knowledge, there is no prior work comparing NCE to contrastive divergence in the case of the RBM, so the following experiments consider all three pairwise comparisons of VNCE, NCE and contrastive divergence.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        RBM EXPERIMENTAL SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Experimental Setup}

Following \citet{carreira2005contrastive}, we use a toy dataset derived from all mutually exclusive $3 \times 3$ patches of the 11,000 $16 \times 16$ images of handwritten digits from the USPS dataset.\footnote{https://cs.nyu.edu/~roweis/data.html} After thresholding the 256 intensity levels at 150, we obtained 275,000 9-dimensional binary vectors. We use this relatively low-dimensional toy dataset in order to make computing the partition function, and hence the log-likelihood, cheap.

For all three estimation methods, we train RBMs with varying number of hidden units: $m \in \{2, 4, 6, 8\}$. For each value of $m$, we use the same initial weights and biases across all three methods, sampling the weights and hidden biases with: $W_{i, j} \sim \mathcal{U}(-4 \sqrt{\frac{6}{d + m}}, 4 \sqrt{\frac{6}{d + m})$ and setting the visible biases to $\frac{\log(p_i)}{1 - \log(p_i)}$ where $p_i$ is the empirical frequency of $u_i = 1$. Setting the visible biases in this way ensures that the initial model approximately matches the empirical marginals, and hence most of training consists in discovering the correlations between the visibles. For VNCE and NCE we must also initialise the scaling parameter; we set it to $-m \log(2) + \sum_{i=0}^{d}(\log(1 - p_i))$, which approximately normalises the initial model (it would exactly normalise it if the weight matrix and hidden biases were initialised to zero).

For contrastive divergence, we use the largest stable learning rate, which is 0.1, a batch size of 100 and train for 100 epochs. For each data point in a batch, we initialise a Markov chain at that point and run it for 1 step to produce 100 `fantasy' samples. Many implementations of CD \citep{hinton2012practical} do not actually take samples of the latent variables on the \emph{final} step of Gibbs sampling. Instead they directly use the probability of the latents to compute the gradient update. Whilst this is a useful property of CD in the case of the RBM, for many other models we will be forced to take samples. Hence, to make our comparison of VNCE and CD fair for the general case, we always take samples.

For both VNCE and NCE, we set the noise distribution over visible units to be a product of marginals. Each marginal is a Bernoulli distribution, with parameter $p_i$ equal to the empirical marginal probability. We set the ratio of noise to data samples, $\nu$, equal to 1. We optimise their objective functions using stochastic gradient ascent (SGD) with the same settings as for contrastive divergence. We also investigated using a non-linear optimiser such as L-BFGS-B, however they tend not to scale well to large datasets. In addition, when using VNCE with L-BFGS-B inside the inner loop of the EM-algorithm, we have found that waiting for the built-in convergence criterion causes slow overall convergence. This can be remedied however by limiting the number of iterations to a small number rather than waiting for convergence.

For VNCE, we update the variational distribution $q$ to equal the model's current posterior over latents at every E-step of the EM algorithm. In the general case $q$ will have its own parameters but for the RBM this is not necessary since we have an analytic expression for $p(\z | \u)$ given in equation \ref{eq: rbm conditional indpendicies} that is efficient to sample from.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                         RBM EXPERIMENTAL RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Experimental Results}
Figure \ref{fig:usps-rbm} shows the learning curves for training RBMs with SGD using a range of latent dimensions: $m \in \{2, 4, 6, 8\}$. In all cases, we see contrastive divergence converges to a higher log-likelihood than NCE or VNCE, although the gap in final log-likelihoods is very small for $m=2$ hidden units, and grows with $m$. NCE and VNCE converge to the same final log-likelihood, showing that the variational approximation has not compromised the quality of the final model. The estimation methods differ significantly in terms of speed: The log-likelihood reached by VNCE at convergence is obtained 2-5x more quickly by NCE, and 10-50x more quickly by contrastive divergence. We note that the final gap between NCE/VNCE and contrastive divergence could be narrowed with a more sophisticated choice of noise distribution or by using a larger ratio of noise to data samples, at the cost of speed.

These results are strongly in favour of contrastive divergence as a method for training RBMs, particularly as we increase the number of hidden units. This observation is consistent with the limited theory that exists for contrastive divergence. As discussed in section \ref{sec:mle and cd}, the difference between the maximum likelihood gradient updates and contrastive divergence updates as stated in equation \ref{eq:cd bias}, can be analytically bounded in the case of the RBM. \citet{bengio2009justifying} provided a bound, later improved by \citet{fischer2011bounding}, which was demonstrated to be both tight and small on a toy problem. More generally the authors show that the bound will be small when the absolute values of the RBM parameters are small. This theory gives us some assurance that contrastive divergence will work well for the RBM, and the theory appears to borne out in practice.

Nevertheless, the amount of bias in contrastive divergence learning can be much larger for particular data generating distributions \citep{marlin2010inductive}, and may be larger for other classes of models. In particular, one might expect that for models where MCMC sampling is less efficient (because we cannot do block sampling, say), that contrastive divergence will be less effective. We investigate these possibilities in subsequent experiments.

\begin{figure}[ht]
\vspace{-5mm}
  \centering
  \includegraphics[width=0.8\linewidth, height=300]{usps-different_num_hiddens.pdf}
  \vspace{-5mm}
    \caption{Learning curves for CD, NCE and VNCE when training Restricted Boltzmann Machines on 3x3 binarised image patches using varying numbers of hidden units. All models here were trained with SGD. The green dashed line is the log-likelihood of the noise distribution used in NCE, which in this case is a product of the empirical marginals.}
    \label{fig:usps-rbm}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        SELF-CONTRASTIVE VNCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Adaptive VNCE}
\citet{goodfellow2014distinguishability} proposed a variation of NCE where, after each update of the parameters $\thetab$, we reset the noise distribution to be equal to the current model. He showed that this scheme, termed `self-contrastive estimation', yields the same gradient updates as maximum likelihood learning. We prove an analog of this result for the latent-variable case, and investigate how to efficiently implement it.

In the the appendix, section \ref{sec:appendix grad of J1}, we derive the gradient of the VNCE objective function, which is:
\begin{align}
\nabla_{\theta}J_1(\theta) & =  
        \Ex \mathbb{E}_{\z \sim q} \left[ \frac{\nu}{r(\x, \z ; \thetab) + \nu}
        \gradtheta \log(\phi(\x, \z; \thetab)) \right] \\
    & - \Ey
        \frac{\nu}{\mathbb{E}_{\z \sim q}[r(\y, \z; \thetab)] + \nu}
        \mathbb{E}_{\z \sim q} \left[ r(\y, \z; \theta) \gradtheta \log(\phi(\y, \z; \thetab)) \right]
\end{align}
where,
\begin{equation}
    r(\u, \z; \theta) = \frac{\pnn(\x, \z ; \thetab)}{q(\z | \u) \pnoise(\u)}
\end{equation}
Now let us suppose that during each E step, not only do we set $q_k(\z | \u) = p(\z | \u; \thetab_k)$, but we also update the noise to be equal to the current model over visibles: $\pnoise(\u) = \pnn(\u; \thetab_k)$. Then,
\begin{equation}
     r(\u, \z; \theta) = \frac{\pnn(\x, \z ; \thetab)}{p(\z | \u; \thetab_k)  \pnn(\u; \thetab_k)} = 1 \ ,
\end{equation}
which implies that the gradient is:
\begin{equation}
    \label{eq:self-contrastive vnce is mle}
    \nabla_{\theta}J_1(\theta) & =  \frac{\nu}{1 + \nu} \Big(
       \Ex \Evar{\x} \left[ \gradtheta \log(\phi(\x, \z; \thetab)) \right]
     - \Ey \Evar{\y}  \left[ \gradtheta \log(\phi(\y, \z; \thetab)) \right] \Big) \ ,
\end{equation}
which is proportional to the maximum likelihood gradient update, as presented in equation \ref{eq:unnormalised latent var mle gradient} in section \ref{sec:mle and cd}.

Now, if during the M-step we only take a single gradient step using equation \ref{eq:self-contrastive vnce is mle}, and then repeat the E-step as stated above, then we have precisely recovered maximum likelihood estimation. If however, we take multiple gradient steps during the M-step, we have a new adaptive version of VNCE that approximates maximum likelihood learning, but is potentially faster. The main problem with this adaptive scheme is that at each E-step, we must acquire noise samples from $\pnn(\u; \thetab_k)$, which could be costly if we have to run an Markov chain Monte Carlo method for a long time. This is the same problem faced by maximum likelihood learning, and one solution there is to only run the Markov chain for $k$ steps, initialising it at a data point. In other words, we can use \emph{contrastive divergence}.

Hence, we have developed an adaptive version of VNCE that includes maximum likelihood and contrastive divergence as special cases. The main question of interest is whether deviating from these special cases, by using multiple gradient updates during the M-step of learning, leads to a faster or more accurate learning method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Present conclusions and remaining work}

Our goal in this preliminary version of the thesis was twofold. Firstly, to give an exposition of an extension to noise-contrastive estimation which applies to unnormalised models with latent variables. Secondly, to evaluate the performance of this new method in the context of two models: a simple unnormalised mixture of gaussians, and a Restricted Boltzmann Machine.

Both evaluations are ongoing, so it is not yet clear how the new method compares to others, such as contrastive divergence. Nevertheless, current results demonstrate that the method can be successfully used for density estimation, at least on simple problems. The remainder of the thesis will need to critically assess how the method scales with model complexity, sample size and diversity of data generating distributions.

In terms of theory, there is also plenty of work remaining. In particular, we have discussed how it would be interesting to explore the convergence properties of this new estimator, as well as performing a mathematical comparison with contrastive divergence. Finally, once the investigation of RBMs is complete, the next logical step is to investigate models with intractable posteriors that are hard to sample from, since our estimation method can, in theory, work in this setting, whilst contrastive divergence cannot.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Appendix}

\begin{lemma}
\label{lemma: f convex}
For non-negative real numbers $a, b$ and $u$, the function 
\begin{equation}
    f(u) = \log(a + bu^{-1})
\end{equation}
is convex.
\end{lemma}
\begin{proof}
Differentiating $f$ twice:
    \begin{align}
        f'(u) = - \frac{b}{au^2 + bu} \hspace{7mm} f''(u) = \frac{b(2au + b)}{(au^2 + bu)^2},
    \end{align}
we see that $f''(u) \geq 0$ since $a, b$ and $u$ are non-negative. Hence $f$ is convex.
\end{proof}

\subsection{Gradient of J1($\thetab$)}
\label{sec:appendix grad of J1}
We can write the VNCE objective function, $J_1(\thetab)$, as follows:

\begin{align}
J_1(\thetab) & =  
              \color{red}{ - \mathbb{E}_{\x} \mathbb{E}_{\z \sim q_k} \left[ \log(\psi_1(\x, \z; \thetab)) \right]} -        \color{blue}{ \nu \mathbb{E}_{\y}  \left[  \log(\psi_2(\y; \thetab)) \right]},
\end{align}
where
\begin{align}
&\color{red}{\psi_1(\x, \z; \thetab) = 1 + \frac{\nu}{r(\x, \z; \thetab)}} \ \ \ \ \ \color{blue}{\psi_2(\y; \thetab) = 1 + \frac{1}{\nu} \mathbb{E}_{\z \sim q_k}[r(\y, \z; \thetab)]} \\
&\text{ and } \ \ \ \ \ \ \ \ \ \ \ r(\u, \z; \thetab) = \frac{\pnn(\x, \z ; \thetab)}{q(\z | \u) \pnoise(\u)}
\end{align}
Now, let us take derivative with respect to $\thetab$:
\begin{align}
\color{red}{ \nabla_{\thetab} \log(\psi_1(\x, \z; \thetab))}
           & = \frac{1}{\psi_1(\x, \z; \thetab)} \nabla_{\thetab} \psi_1(\x, \z; \thetab) \\
           & = \frac{1}{\psi_1(\x, \z; \thetab)} \frac{-\nu}{r(\x, \z; \thetab)^2} \nabla_{\thetab} r(\x, \z; \thetab) \\
           & = \frac{1}{\psi_1(\x, \z; \thetab)} \frac{-\nu}{r(\x, \z; \thetab)^2} \frac{\nabla_{\thetab} \phi(\x, \z; \thetab)}{q_k(\z \ | \ \x)p_{\y}(\x)}.
           \label{eq:grad log psi 1}
\end{align}
It is convenient to re-express $\nabla_{\thetab} \phi(\x, \z; \thetab)$ as follows:
\begin{align}
\nabla_{\thetab} \phi(\x, \z; \thetab) & = \nabla_{\thetab} \exp ( \log ( \phi(\x, \z; \thetab) ) ) \\
          & = \left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right] \exp ( \log ( \phi(\x, \z; \thetab) ) )\\
          & = \left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right] \phi(\x, \z; \thetab).
\end{align}
Plugging this back into equation \ref{eq:grad log psi 1}, we get:
\begin{align}
\color{red}{ \nabla_{\thetab} \log(\psi_1(\x, \z; \thetab))}
    & = \frac{1}{\psi_1(\x, \z; \thetab)}
        \frac{-\nu}{r(\x, \z; \thetab)^2}
        \frac{\left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right] \phi(\x, \z; \thetab)}{q_k(\z \ | \ \x)p_{\y}(\x)} \\
    & = \frac{1}{\psi_1(\x, \z; \thetab)}
        \frac{-\nu}{r(\x, \z; \thetab)}
        \left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right] \\
    & = \color{red}\frac{- \nu}{r(\x, \z ; \thetabb) + \nu}
        \left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right] \\
\end{align}
Where in the final line we used the fact that $\color{red}{\psi_1(\x, \z; \thetab) - 1 = \frac{\nu}{r(\x, \z; \thetab)}}$.

Now we find the derivative of the second (blue) term of $J_1^K(\thetab)$. Recalling that $\color{blue}{\psi_2(\y; \thetab) = 1 + \frac{1}{\nu} \mathbb{E}_{\z \sim q_k}[r(\y, \z; \thetab)]}$, we have:
\begin{align}
\color{blue}{\nabla_{\thetab} \log(\psi_2(\y; \thetab))}
    & = \frac{1}{\psi_2(\y; \thetab)} \nabla_{\thetab} \psi_2(\y; \thetab) \\
    & = \frac{1}{\psi_2(\y; \thetab)}
        \frac{1}{\nu} \mathbb{E}_{\z \sim q_k}
        \left[ \frac{\nabla_{\thetab} \phi(\y, \z; \thetab)}{q_k(\z \ | \ \y)p_{\y}(\y)} \right] \\  
    & = \color{blue}{\frac{1}{\nu}
        \frac{1}{\psi_2(\y; \thetab)}
        \mathbb{E}_{\z \sim q_k} \left[ r(\y, \z; \thetab) \left[ \nabla_{\thetab} \log(\phi(\y, \z; \thetab)) \right] \right]} \\
\end{align}
Putting this all together, we arrive at:
\begin{align}
\nabla_{\thetab}(J_1^k(\thetab)) & =  
    \color{red}{  \mathbb{E}_{\x} \mathbb{E}_{\z \sim q_k} \frac{\nu}{r(\x, \z ; \thetab) + \nu}
        \left[ \nabla_{\thetab} \log(\phi(\x, \z; \thetab)) \right]} \\
    &\color{blue}{ - \mathbb{E}_{\y}
                 \frac{1}{1 + \frac{1}{\nu} \mathbb{E}_{\z \sim q_k}[r(\y, \z; \thetab)]}
                \mathbb{E}_{\z \sim q_k} \left[ r(\y, \z; \thetab) \left[ \nabla_{\thetab} \log(\phi(\y, \z; \thetab)) \right] \right]} \\
\end{align}






\newpage
\bibliographystyle{plainnat}
\bibliography{interim_report.bib}

\end{document}  
