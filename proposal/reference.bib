% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Andrieu2003,
  Title                    = {{A}n {I}ntroduction to {MCMC} for {M}achine {L}earning},
  Author                   = {Andrieu, C. and deFreitas, N. and Doucet, A. and Jordan, M.I.},
  Journal                  = {Machine Learning},
  Year                     = {2003},
  Pages                    = {5--43},
  Volume                   = {50},

  File                     = {Andrieu2003.pdf:Andrieu2003.pdf:PDF},
  Owner                    = {gutmann},
  Timestamp                = {2011.04.22}
}

@InProceedings{Gutmann2013b,
  Title                    = {Estimation of unnormalized statistical models without numerical integration},
  Author                   = {Gutmann, M.U. and Hyv\"arinen, A.},
  Booktitle                = {Proc Workshop on Information Theoretic Methods in Science and Engineering},
  Year                     = {2013},

  File                     = {Gutmann2013b.pdf:gutmann_publ/Gutmann2013b.pdf:PDF},
  Owner                    = {gutmann},
  Timestamp                = {2013.08.19}
}

@Article{Gutmann2012a,
  Title                    = {{N}oise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
  Author                   = {Gutmann, M.U. and Hyv\"arinen, A.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2012},
  Pages                    = {307--361},
  Volume                   = {13},

  File                     = {Gutmann2012a.pdf:gutmann_publ/Gutmann2012a.pdf:PDF},
  Owner                    = {gutmann},
  Timestamp                = {2012.03.03}
}

@Article{Gutmann2013,
  Title                    = {A three-layer model of natural image statistics},
  Author                   = {Gutmann, Michael U. and Hyv\"arinen, A.},
  Journal                  = {Journal of Physiology-Paris},
  Year                     = {2013},

  Month                    = nov,
  Number                   = {5},
  Pages                    = {369--398},
  Volume                   = {107},

  Booktitle                = {Special issue: Neural Coding and Natural Image Statistics},
  Doi                      = {10.1016/j.jphysparis.2013.01.001},
  File                     = {Reprint:gutmann_publ/Gutmann2013_reprint.pdf:PDF;Preprint:gutmann_publ/Gutmann2013.pdf:PDF},
  ISSN                     = {0928-4257},
  Keywords                 = {Natural images, Probabilistic modeling, Visual processing, Selectivity, Invariance, Sparse coding, Deep learning},
  Owner                    = {gutmann},
  Timestamp                = {2013.10.17},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0928425713000028}
}

@InProceedings{Mnih2012,
  Title                    = {{A} fast and simple algorithm for training neural probabilistic language models},
  Author                   = {Mnih, Andriy and Teh. Yee Whye},
  Booktitle                = {ICML},
  Year                     = {2012},

  Abstract                 = {Neural probabilistic language models (NPLMs) have recently superseded smoothed n-gram models as the best-performing model class for language modelling. Unfortunately, the adoption of NPLMs is held back by their notoriously long training times, which can be measured in weeks even for moderately-sized datasets. These are a consequence of the models being explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results in the Microsoft Research Sentence Completion Challenge.},
  File                     = {Mnih2012.pdf:Mnih2012.pdf:PDF},
  Owner                    = {gutmann},
  Timestamp                = {2012.05.24}
}

@Conference{Tschiatschek2016,
  Title                    = {Learning probabilistic submodular diversity models via noise contrastive estimation},
  Author                   = {Tschiatschek, S. and Djolonga, J. and Krause, A.},
  Booktitle                = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  Year                     = {2016},

  Owner                    = {mgutmann},
  Timestamp                = {2016.12.21}
}

@Conference{Zoph2016,
  Title                    = {Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies},
  Author                   = {Zoph, B. and Vaswani, A. and May, J. and Knight, K.},
  Booktitle                = {Proc. NAACL},
  Year                     = {2016},

  Owner                    = {mgutmann},
  Timestamp                = {2016.12.21}
}

